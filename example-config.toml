FsApiToken = "<your FullStory API token>"
Backoff = "30s"
BackoffStepsMax = 8
CheckInterval = "30m"
TmpDir = "/tmp"
Warehouse="local"
GroupFilesByDay = false
SaveAsJson = false

[s3]
# bucket that will be used to stage files into Redshift
Bucket = ""
# region of the above bucket
Region = "us-east-2"
# timeout for copying export files from the local machine to S3
Timeout = "5m"
# upload data file only (i.e. skip load to Redshift)
S3Only = true

[redshift]
User = "<your user>"
Password = "<password>"
Host = "<redshift host details>.redshift.amazonaws.com"
Port = "5439"
DB = "dev"
# the table where the export data will be written
ExportTable = "fsexport"
# metadata table that holds info on export history
SyncTable   = "fssync"
# IAM role associated with redshift
Credentials = "aws_iam_role=arn:aws:iam::<...>"
VarCharMax = 65535
DatabaseSchema = "public"

[gcs]
Bucket = "<your bucket>"
# upload data file only (i.e. skip load to BigQuery)
GCSOnly = true

[bigquery]
Project = "<your project>"
Dataset = "<your dataset>"
ExportTable = "fs_export"
SyncTable = "fs_sync"
# The amount of time after which the partitions will expire
# Valid time units are "s", "m", "h" (seconds, minutes, hours).
# For example, "720h" would expire the partitions after 30 days.
# If this value is omitted or "0", then the partitions will not expire.
PartitionExpiration = "0"

[local]
SaveDir = "<Path to your local folder to save files to>"
StartTime = <Start time for data exports in the following format: 2018-12-27T18:30:00Z>
UseStartTime = true

